{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MartinVoelkl94/Python_compendium/blob/main/mv_functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEh9H_mViNsg"
      },
      "source": [
        "contains utility functions.\n",
        "\n",
        "mv_functions.ipynb is meant to be used for looking up, editing or adding functions while mv_functions.py is used to import functions to other scripts or notebooks (use 'import mv_functions as mv').\n",
        "\n",
        "The last cell of mv_functions.ipynb contains code that backs up the current mv_functions.py file and converts mv_functions.ipynb into a new mv_functions.py."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# imports"
      ],
      "metadata": {
        "id": "X5776VC-Ht46"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ndcqlRcFiNsi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7b9fa53-6d2b-4344-9f73-ddfb02aadde9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import types\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import pickle\n",
        "\n",
        "\n",
        "#this part is not meant to be executed after conversion to a .py file\n",
        "if 'colab' in get_ipython().config['IPKernelApp']['kernel_class']:\n",
        "    # mounting to drive folder\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    #acces functions from mv_functions.py\n",
        "    sys.path.append('/content/drive/MyDrive/coding/Python/Compendium')\n",
        "    os.chdir('/content/drive/MyDrive/coding/Python/Compendium')\n",
        "    import mv_functions as mv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# mv.tree"
      ],
      "metadata": {
        "id": "jP1_8PPHHRZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tree(data, name='data', indent='|   '):\n",
        "    \"\"\"\n",
        "    gives a condensed overview of the content of an object in a form resembling\n",
        "    a folder tree. Made to be used in data exploration or when investigating a \n",
        "    new algorithm. It has similar usecases as the basic type() function but in \n",
        "    addition it also gives more information on certain common data types an is\n",
        "    able to show multiple layers of nested objects.\n",
        "\n",
        "    Parameters:\n",
        "    data: an object identifiable ny type()\n",
        "    name: optional. the name of the current object\n",
        "    indent: used to modify visual presentation of the output\n",
        "\n",
        "    Returns:\n",
        "    prints to output instead of returning\n",
        "    \"\"\"\n",
        "    name = [name]\n",
        "    level = 0  #tracks progress through layers of nested objects\n",
        "    _tree_check_type(data, name, level, indent)   \n",
        "    \n",
        "\n",
        "def _tree_check_type(current_data, name, level, indent):\n",
        "    #used by tree function to check the type of current_data\n",
        "\n",
        "    indents = level*indent\n",
        "    current_data_name = ''.join(name)\n",
        "    \n",
        "    #the following if-statements check for common data types and then go a\n",
        "    #level deeper when encountering a list, dict, array or dataframe\n",
        "    if isinstance(current_data, list):\n",
        "        if level == 0:\n",
        "            print(f'{indents}list:')\n",
        "        else:\n",
        "            print(f'{indents}list: {current_data_name}')\n",
        "        level += 1\n",
        "        _tree_open_list(current_data, name, level, indent)\n",
        "\n",
        "    elif isinstance(current_data, dict):\n",
        "        if level == 0:\n",
        "            print(f'{indents}dictionary:')\n",
        "        else:\n",
        "            print(f'{indents}dictionary: {current_data_name}')\n",
        "        level += 1\n",
        "        _tree_open_dict(current_data, name, level, indent)\n",
        "\n",
        "    elif isinstance(current_data, np.ndarray):\n",
        "        if level == 0:\n",
        "            print(f'{indents}np.ndarray:')\n",
        "        else:\n",
        "            print(f'{indents}np.ndarray: {current_data_name}')\n",
        "        level += 1\n",
        "        _tree_open_np_ndarray(current_data, name, level, indent)\n",
        "    \n",
        "    elif isinstance(current_data, pd.core.frame.DataFrame):\n",
        "        if level == 0:\n",
        "            print(f'{indents}dataframe:')\n",
        "        else:\n",
        "            print(f'{indents}dataframe: {current_data_name}')\n",
        "        level += 1\n",
        "        _tree_open_pd_dataframe(current_data, name, level, indent)\n",
        "    \n",
        "    else:\n",
        "        print(f'{indents}{str(type(current_data))[8:-2]}')\n",
        "\n",
        "\n",
        "def _tree_open_list(current_data, name, level, indent):\n",
        "    #used by tree function to open and display contents of lists.\n",
        "\n",
        "    counter = {}\n",
        "    for ind in range(len(current_data)):\n",
        "        if str(type(current_data[ind]))[8:-2] in counter.keys():\n",
        "            counter[str(type(current_data[ind]))[8:-2]] += 1\n",
        "        else:\n",
        "            counter[str(type(current_data[ind]))[8:-2]] = 1\n",
        "    \n",
        "    for key in counter.keys():\n",
        "        print(f'{level*indent}{key}: {counter[key]} times')\n",
        "\n",
        "\n",
        "def _tree_open_dict(current_data, name, level, indent):\n",
        "    #used by tree function to open and display contents of dictionaries.\n",
        "\n",
        "    for key in current_data.keys():\n",
        "        if isinstance(key, str):\n",
        "            name.append(f'[\"{key}\"]')\n",
        "        else:\n",
        "            name.append(f'[{str(key)}]')\n",
        "        _tree_check_type(current_data[key], name, level, indent)\n",
        "        name.pop()\n",
        "\n",
        "\n",
        "def _tree_open_np_ndarray(current_data, name, level, indent):\n",
        "    #used by tree function to open and display contents of numpy arrays.\n",
        "\n",
        "    current_data_name = ''.join(name)\n",
        "    \n",
        "    if current_data.shape[0] == 1:\n",
        "        cols = f'[0,0:{len(current_data[0,:])}]'\n",
        "        print(f'{level*indent}1 col: {current_data_name}[0,:]')\n",
        "        print(f'{level*indent}{len(current_data[0,:])} rows: {current_data_name}{cols}')\n",
        "        \n",
        "    elif current_data.shape[0] >= 2:\n",
        "        rows = f'[0:{len(current_data[:,0])},:]'\n",
        "        print(f'{level*indent}{len(current_data[:,0])} rows: {current_data_name}{rows}')\n",
        "        \n",
        "        cols = f'[:,0:{len(current_data[0,:])}]'\n",
        "        print(f'{level*indent}{len(current_data[0,:])} cols: {current_data_name}{cols}')\n",
        "        \n",
        "    else:\n",
        "        print(f'{level*indent}shape: {current_data.shape}')\n",
        "\n",
        "\n",
        "def _tree_open_pd_dataframe(current_data, name, level, indent):\n",
        "    #used by tree function to open and display contents of pandas dataframes.\n",
        "\n",
        "    for colname in list(current_data):\n",
        "        current_data_name = ''.join(name)\n",
        "        n_values = len(current_data[colname])\n",
        "        if isinstance(colname, str):\n",
        "            print(f'{level*indent}{n_values} values in: {current_data_name}[\"{colname}\"]')\n",
        "        else:\n",
        "            print(f'{level*indent}{n_values} values in: {current_data_name}[{colname}]')"
      ],
      "metadata": {
        "id": "a1rPDk3T01TW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# mv.save"
      ],
      "metadata": {
        "id": "IL3ku_3bHWm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save(data, path=None, readme='no readme found',\n",
        "         supp={}, overwrite=False, verbose=True):\n",
        "    \"\"\"\n",
        "    Makes working with various objects a little faster and more convenient.\n",
        "    Not meant for use in production ready code. Saves any type of python object\n",
        "    thats compatible with the pickle library as a file while offering some\n",
        "    additional convenience:\n",
        "        -one function for all data types\n",
        "        -therefore can be used when type of output is unknown beforehand\n",
        "        -option to include a readme string to explain the data\n",
        "        -option to include additional supplementary data\n",
        "        -if chosen or default (data.pkl) filename already exists it increments\n",
        "            the number in the filename instead of overwriting the old file\n",
        "\n",
        "    Parameters:\n",
        "    data: any object compatible with the pickle library.\n",
        "    path: optional. filename or filepath to save the data as. if none is\n",
        "        provided, the data is saved in the current folder as data.pkl.\n",
        "        if the chosen or default name is already taken, ascending numbers\n",
        "        (up to 1000) are added until the name is valid. its not necessary\n",
        "        (but possible) to add the '.pkl' extension when calling the function.\n",
        "    readme: optional. a string that can be saved with the data, for example\n",
        "        to explain where the data came from or how it was generated.\n",
        "        when loading the data   with the corresponding function mv.load the\n",
        "        string can be recalled.\n",
        "    supp: optional. a dictionary containing any additional object/s to be saved\n",
        "        together with the main data, for example the source of the data or the\n",
        "        code that produced it.\n",
        "    overwrite: whether or not existing files should be overwritten if they\n",
        "        have the same name as the one chosen for the data to save.\n",
        "    verbose: if confirmation and location of the saved file should be printed\n",
        "    \"\"\"\n",
        "\n",
        "    if path == None: #neither directory nor filename provided\n",
        "        filename = 'data'\n",
        "        directory = os.getcwd()\n",
        "\n",
        "    elif '/' in path:  #directory provided\n",
        "        directory = '/'.join(path.split('/')[:-1])\n",
        "        if path.split('/')[-1] == '':  #directory provided but no filename\n",
        "            filename = 'data'\n",
        "        else:  #directory and filename provided\n",
        "            filename = path.split('/')[-1]\n",
        "\n",
        "    else:  #filename provided but no directory\n",
        "        directory = os.getcwd()\n",
        "        filename = path\n",
        "\n",
        "    #cut of extension (if one is given)\n",
        "    filename = filename.split('.')[0]\n",
        "\n",
        "    #combine directory and filename:\n",
        "    save_path = f'{directory}/{filename}.pkl'\n",
        "\n",
        "    #put data, readme and supplements into dictionary\n",
        "    save_dict = {'data': data, 'readme': readme, 'supplementary': supp}\n",
        "\n",
        "\n",
        "    #if filename doesnt exist or should be overwritten:\n",
        "    if not os.path.exists(save_path) or overwrite == True:\n",
        "        pass\n",
        "    #increment filenumber as to not overwrite existing files instead\n",
        "    else:\n",
        "        existing_filenames = os.listdir(directory)\n",
        "        for i in range(1000):\n",
        "            save_name = f'{filename}{str(i)}.pkl'\n",
        "            if save_name not in existing_filenames:\n",
        "                save_path = f'{directory}/{save_name}'\n",
        "                break\n",
        "\n",
        "    #save data\n",
        "    with open(save_path, 'wb') as file:\n",
        "        pickle.dump(save_dict, file)\n",
        "        if verbose:\n",
        "            print('data saved in: ', save_path)"
      ],
      "metadata": {
        "id": "AlUe3D9gSMC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# mv.load"
      ],
      "metadata": {
        "id": "7gFDRTo8HeKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load(path='data', readme=False, supp=False, verbose=False):\n",
        "    \"\"\"\n",
        "    loads objects saved with mv.save.\n",
        "\n",
        "    Parameters:\n",
        "    path: optional. filename or filepath of the object to load. if non is given,\n",
        "        the default path of mv.save (data.pkl) is used.\n",
        "    readme: wether to load the readme string saved with the object\n",
        "    supp: wether to load the dictionary of supplements saved with the object\n",
        "    verbose: switches 'commentary' on or off\n",
        "    \"\"\"\n",
        "    \n",
        "    if '.pkl' not in path and '.pckl' not in path:\n",
        "        path = f'{path}.pkl'\n",
        "\n",
        "    with open(path, 'rb') as file:\n",
        "        save_dict = pickle.load(file)\n",
        "        \n",
        "    if readme:\n",
        "        print('readme:\\n', save_dict['readme'])\n",
        "        \n",
        "    if supp:\n",
        "        if verbose:\n",
        "            print('supplementary information loaded from: ', path)\n",
        "        return(save_dict['supplementary'])\n",
        "    else:\n",
        "        if verbose:\n",
        "            print('data without supplementary information loaded from: ', path)\n",
        "        return(save_dict['data'])"
      ],
      "metadata": {
        "id": "DUuAIftoHc_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# mv.samples"
      ],
      "metadata": {
        "id": "3Ol4ZPRwHg2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def samples(dirname='samples'):\n",
        "    \"\"\"\n",
        "    creates a folder with sample files in various formats for use in\n",
        "    Python_compendium.ipynb or testing functions.\n",
        "    \"\"\"\n",
        "    if os.path.exists(dirname):\n",
        "        print(f'folder \"{dirname}\" found')\n",
        "    else:\n",
        "        os.mkdir(dirname)\n",
        "        print(f'created folder \"{dirname}\"')\n",
        "        \n",
        "\n",
        "\n",
        "    #list as .txt\n",
        "    list1 = [f'random number: {random.randint(0, 100)}' for x in range(20)]\n",
        "\n",
        "    with open(f'{dirname}/list.txt', 'w') as file:\n",
        "        for line in list1:\n",
        "            file.write(f'{line}\\n')\n",
        "        print(f'created file \"{dirname}/list.txt\"')\n",
        "\n",
        "\n",
        "\n",
        "    #numpy arrays as .npy, .txt, .csv\n",
        "    numpy_array = np.array([[x*y for x in range(5)] for y in range(6)])\n",
        "\n",
        "    np.save(f'{dirname}/array.npy', numpy_array)\n",
        "    print(f'created file \"{dirname}/array.npy\"')\n",
        "    np.savetxt(f'{dirname}/array.txt', numpy_array, delimiter=',')\n",
        "    print(f'created file \"{dirname}/array.txt\"')\n",
        "    np.savetxt(f'{dirname}/array.csv', numpy_array, delimiter=',')\n",
        "    print(f'created file \"{dirname}/array.csv\"')\n",
        "\n",
        "\n",
        "\n",
        "    #pandas dataframes as .csv, .xlsx\n",
        "    df = pd.DataFrame([(1, 2.0, 'Hello', True), (2, 3.0, 'World', False)],\n",
        "                    index=['A', 'B'],\n",
        "                    columns=[1, 2, 3, 4])\n",
        "\n",
        "    df.to_csv(f'{dirname}/df.csv', index=True)\n",
        "    print(f'created file \"{dirname}/df.csv\"')\n",
        "    df.to_excel(f'{dirname}/df.xlsx', index=True)\n",
        "    print(f'created file \"{dirname}/df.xlsx\"')\n",
        "\n",
        "\n",
        "\n",
        "    #dictionary as .pkl\n",
        "    dict1 = {'name': 'Herbert', 'age': 22, 'height': 172}\n",
        "\n",
        "    with open(f'{dirname}/dict.pkl', 'wb') as file:\n",
        "            pickle.dump(dict1, file)\n",
        "            print(f'created file \"{dirname}/dict.pkl\"')\n",
        "\n",
        "\n",
        "\n",
        "    #nested object as .pkl\n",
        "    integer = 1\n",
        "    boolean = True\n",
        "    dict1 = {'name': 'Herbert', 'age': 22, 'height': 172, 1: 'id'}\n",
        "    list1 = [0, 1, 2, 'abc']\n",
        "    list2 = [[0,1,2], ['a', 'b'], [True, True, False], 0, 0.2, 'a', True, dict1]\n",
        "    array1 = [(1, 2.0, 'Hello'), (2, 3.0, 'World')]\n",
        "    array2 = np.array(array1)\n",
        "    dict2 = {'id': 'a5', 'contents': list2, 'date': 2021}\n",
        "    df1 = pd.DataFrame(array1, index=['X', 'Y'], columns=['A', 'B', 3])\n",
        "\n",
        "    nested = {'int': integer, 'boolean': boolean, 'dict1': dict1,\n",
        "            'dict2': dict2, 'list1': list1,'list2': list2,\n",
        "            'array': array1, 'array2': array2, 'dataframe': df1}\n",
        "\n",
        "    with open(f'{dirname}/nested.pkl', 'wb') as file:\n",
        "            pickle.dump(nested, file)\n",
        "            print(f'created file \"{dirname}/nested.pkl\"')"
      ],
      "metadata": {
        "id": "aLxcVi5qX1p_"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# mv.bin"
      ],
      "metadata": {
        "id": "o8gfMHaOR1kb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bin(path=None, timespan=10, delete=False, bin_dir='bin', names=False,\n",
        "        extensions=['.py', '.txt', '.csv', '.xlsx', '.npy', '.pkl']):\n",
        "    \"\"\"\n",
        "    looks in current working directory (default) or any given filepath for any\n",
        "    files modified within a specified timespan (default 10s) and puts then in\n",
        "    a trashbin folder. If delete is set to True it will delete the folder.\n",
        "    In default mode it only deletes files with the extensions .py, .txt, .csv,\n",
        "    .xlsx, .npy, .pkl.\n",
        "    \"\"\"\n",
        "\n",
        "    now = time.time()\n",
        "    if path == None:\n",
        "        path = os.getcwd()\n",
        "    if not os.path.exists(f'{path}/{bin_dir}'):\n",
        "        os.mkdir(f'{path}/{bin_dir}')\n",
        "\n",
        "    all_files = os.listdir(path)\n",
        "    files = []\n",
        "    for file in all_files:\n",
        "        for extension in extensions:\n",
        "            if extension in file:\n",
        "                files.append(file)\n",
        "                break\n",
        "\n",
        "    moved = []\n",
        "    for file in files:\n",
        "        mod_time = os.path.getmtime(f'{path}/{file}')\n",
        "        if now - mod_time < timespan:\n",
        "            shutil.move(f'{path}/{file}', f'{path}/{bin_dir}/{file}')\n",
        "            moved.append(file)\n",
        "    print(f'{len(moved)} files moved to {path}/{bin_dir}')\n",
        "\n",
        "    if delete or len(os.listdir(f'{path}/{bin_dir}')) == 0:\n",
        "        shutil.rmtree(f'{path}/{bin_dir}')\n",
        "        print(f'deleted {path}/{bin_dir}')\n",
        "\n",
        "    if names:\n",
        "        return moved"
      ],
      "metadata": {
        "id": "OVucPeepR5yC"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# update mv_functions.py"
      ],
      "metadata": {
        "id": "NsruRcayHl0J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ned15R0iNsr",
        "outputId": "7452cf89-9994-4b0e-ee72-bff9cf7b8a6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saving old mv_functions file as: mv_functions_saves/2023-04-17 18:49:48.586195.py\n",
            "[NbConvertApp] Converting notebook mv_functions.ipynb to script\n",
            "[NbConvertApp] Writing 14613 bytes to mv_functions.py\n"
          ]
        }
      ],
      "source": [
        "#this code backs up the current mv_functions.py file and converts \n",
        "#mv_functions.ipynb into a new mv_functions.py.\n",
        "#lastly it deletes the lines containing itself from the file.\n",
        "\n",
        "os.chdir('/content/drive/MyDrive/coding/Python/Compendium')\n",
        "if os.path.exists('mv_functions.py'):\n",
        "  save_path = f'mv_functions_saves/{datetime.datetime.now()}.py'\n",
        "  shutil.copy('mv_functions.py', save_path)\n",
        "  print(f'saving old mv_functions file as: {save_path}')\n",
        "\n",
        "!jupyter nbconvert --to script mv_functions.ipynb\n",
        "\n",
        "with open('mv_functions.py', 'r') as file:\n",
        "    text = file.readlines()\n",
        "with open('mv_functions.py', 'w') as file:\n",
        "    file.write('#this file was created from mv_functions.ipynb using nbconvert\\n')\n",
        "    for line in text[0:-25]:  \n",
        "        file.write(line)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}